{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Whsg1XX_OZs6"
      },
      "source": [
        "# Set up for dataset and model\n",
        "\n",
        "Package installation, loading, and dataloaders. There's also a resnet18 model defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R1domTvnONqD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to cifar10_data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 98347875.29it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting cifar10_data/cifar-10-python.tar.gz to cifar10_data/\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# !pip install tensorboardX\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "# from tensorboardX import SummaryWriter\n",
        "\n",
        "use_cuda = True\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "batch_size = 64\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "## Dataloaders\n",
        "train_dataset = datasets.CIFAR10('cifar10_data/', train=True, download=True, transform=transforms.Compose(\n",
        "    [transforms.ToTensor()]\n",
        "))\n",
        "test_dataset = datasets.CIFAR10('cifar10_data/', train=False, download=True, transform=transforms.Compose(\n",
        "    [transforms.ToTensor()]\n",
        "))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no input normalization\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PreActResNet(\n",
              "  (normalize): Normalize()\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): PreActBlock(\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    )\n",
              "    (1): PreActBlock(\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): PreActBlock(\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (1): PreActBlock(\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): PreActBlock(\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (1): PreActBlock(\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): PreActBlock(\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (1): PreActBlock(\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    )\n",
              "  )\n",
              "  (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def tp_relu(x, delta=1.):\n",
        "    ind1 = (x < -1. * delta).float()\n",
        "    ind2 = (x > delta).float()\n",
        "    return .5 * (x + delta) * (1 - ind1) * (1 - ind2) + x * ind2\n",
        "\n",
        "def tp_smoothed_relu(x, delta=1.):\n",
        "    ind1 = (x < -1. * delta).float()\n",
        "    ind2 = (x > delta).float()\n",
        "    return (x + delta) ** 2 / (4 * delta) * (1 - ind1) * (1 - ind2) + x * ind2\n",
        "\n",
        "class Normalize(nn.Module):\n",
        "    def __init__(self, mu, std):\n",
        "        super(Normalize, self).__init__()\n",
        "        self.mu, self.std = mu, std\n",
        "\n",
        "    def forward(self, x):\n",
        "        return (x - self.mu) / self.std\n",
        "\n",
        "class IdentityLayer(nn.Module):\n",
        "    def forward(self, inputs):\n",
        "        return inputs\n",
        "    \n",
        "class PreActBlock(nn.Module):\n",
        "    '''Pre-activation version of the BasicBlock.'''\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, bn, learnable_bn, stride=1, activation='relu'):\n",
        "        super(PreActBlock, self).__init__()\n",
        "        self.collect_preact = True\n",
        "        self.activation = activation\n",
        "        self.avg_preacts = []\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes, affine=learnable_bn) if bn else IdentityLayer()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=not learnable_bn)\n",
        "        self.bn2 = nn.BatchNorm2d(planes, affine=learnable_bn) if bn else IdentityLayer()\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=not learnable_bn)\n",
        "\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=not learnable_bn)\n",
        "            )\n",
        "\n",
        "    def act_function(self, preact):\n",
        "        if self.activation == 'relu':\n",
        "            act = F.relu(preact)\n",
        "        elif self.activation[:6] == '3prelu':\n",
        "            act = tp_relu(preact, delta=float(self.activation.split('relu')[1]))\n",
        "        elif self.activation[:8] == '3psmooth':\n",
        "            act = tp_smoothed_relu(preact, delta=float(self.activation.split('smooth')[1]))\n",
        "        else:\n",
        "            assert self.activation[:8] == 'softplus'\n",
        "            beta = int(self.activation.split('softplus')[1])\n",
        "            act = F.softplus(preact, beta=beta)\n",
        "        return act\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.act_function(self.bn1(x))\n",
        "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x  # Important: using out instead of x\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(self.act_function(self.bn2(out)))\n",
        "        out += shortcut\n",
        "        return out\n",
        "\n",
        "class PreActResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, n_cls, cuda=True, half_prec=False,\n",
        "        activation='relu', fts_before_bn=False, normal='none'):\n",
        "        super(PreActResNet, self).__init__()\n",
        "        self.bn = True\n",
        "        self.learnable_bn = True  # doesn't matter if self.bn=False\n",
        "        self.in_planes = 64\n",
        "        self.avg_preact = None\n",
        "        self.activation = activation\n",
        "        self.fts_before_bn = fts_before_bn\n",
        "        if normal == 'cifar10':\n",
        "            self.mu = torch.tensor((0.4914, 0.4822, 0.4465)).view(1, 3, 1, 1)\n",
        "            self.std = torch.tensor((0.2471, 0.2435, 0.2616)).view(1, 3, 1, 1)\n",
        "        else:\n",
        "            self.mu = torch.tensor((0.0, 0.0, 0.0)).view(1, 3, 1, 1)\n",
        "            self.std = torch.tensor((1.0, 1.0, 1.0)).view(1, 3, 1, 1)\n",
        "            print('no input normalization')\n",
        "        if cuda:\n",
        "            self.mu = self.mu.cuda()\n",
        "            self.std = self.std.cuda()\n",
        "        if half_prec:\n",
        "            self.mu = self.mu.half()\n",
        "            self.std = self.std.half()\n",
        "\n",
        "        self.normalize = Normalize(self.mu, self.std)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=not self.learnable_bn)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.bn = nn.BatchNorm2d(512 * block.expansion)\n",
        "        self.linear = nn.Linear(512*block.expansion, n_cls)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, self.bn, self.learnable_bn, stride, self.activation))\n",
        "            # layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        for layer in [*self.layer1, *self.layer2, *self.layer3, *self.layer4]:\n",
        "            layer.avg_preacts = []\n",
        "\n",
        "        out = self.normalize(x)\n",
        "        out = self.conv1(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        if return_features and self.fts_before_bn:\n",
        "            return out.view(out.size(0), -1)\n",
        "        out = F.relu(self.bn(out))\n",
        "        if return_features:\n",
        "            return out.view(out.size(0), -1)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def PreActResNet18(n_cls, cuda=True, half_prec=False, activation='relu', fts_before_bn=False,\n",
        "    normal='none'):\n",
        "    #print('initializing PA RN-18 with act {}, normal {}'.format())\n",
        "    return PreActResNet(PreActBlock, [2, 2, 2, 2], n_cls=n_cls, cuda=cuda, half_prec=half_prec,\n",
        "        activation=activation, fts_before_bn=fts_before_bn, normal=normal)\n",
        "\n",
        "\n",
        "# intialize the model\n",
        "model = PreActResNet18(10, cuda=True, activation='softplus1').to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCmWfZHTO8Oo"
      },
      "source": [
        "# Implement the Attacks\n",
        "\n",
        "Functions are given a simple useful signature that you can start with. Feel free to extend the signature as you see fit.\n",
        "\n",
        "You may find it useful to create a 'batched' version of PGD that you can use to create the adversarial attack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EZjvA49yONqP"
      },
      "outputs": [],
      "source": [
        "def pgd_linf_untargeted(model, x, labels, k, eps, eps_step):\n",
        "    model.eval()\n",
        "    ce_loss = torch.nn.CrossEntropyLoss()\n",
        "    adv_x = x.clone().detach()\n",
        "    adv_x.requires_grad_(True) \n",
        "    for _ in range(k):\n",
        "        adv_x.requires_grad_(True)\n",
        "        model.zero_grad()\n",
        "        output = model(adv_x)\n",
        "        # TODO: Calculate the loss\n",
        "        loss = ce_loss(output, labels)\n",
        "        loss.backward()\n",
        "        # TODO: compute the adv_x\n",
        "        # find delta, clamp with eps\n",
        "        x_perturbation = eps_step * adv_x.grad.sign()\n",
        "        adv_x = adv_x.detach() + x_perturbation\n",
        "        delta = adv_x - x\n",
        "        delta = torch.clamp(delta, min=-eps, max=eps)\n",
        "        adv_x = torch.clamp(x + delta, min=0, max=1).detach()\n",
        "\n",
        "    return adv_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pgd_l2_untargeted(model, x, labels, k, eps, eps_step):\n",
        "    model.eval()\n",
        "    ce_loss = torch.nn.CrossEntropyLoss()\n",
        "    adv_x = x.clone().detach()\n",
        "    adv_x.requires_grad_(True) \n",
        "    for _ in range(k):\n",
        "        adv_x.requires_grad_(True)\n",
        "        model.zero_grad()\n",
        "        output = model(adv_x)\n",
        "        batch_size = x.size()[0]\n",
        "        # TODO: Calculate the loss\n",
        "        loss = ce_loss(output, labels)\n",
        "        loss.backward()\n",
        "        grad = adv_x.grad.sign()\n",
        "        # TODO: compute the adv_x\n",
        "        # find delta, clamp with eps, project delta to the l2 ball\n",
        "        # HINT: https://github.com/Harry24k/adversarial-attacks-pytorch/blob/master/torchattacks/attacks/pgdl2.py \n",
        "        grad_norms = torch.norm(grad.view(batch_size, -1), p=2, dim=1)\n",
        "        grad = grad / grad_norms.view(batch_size, 1, 1, 1)\n",
        "\n",
        "        # Take a step in the direction of the gradient\n",
        "        adv_x = adv_x.detach() + eps_step * grad\n",
        "\n",
        "        # Project back into L2 epsilon-ball\n",
        "        delta = adv_x - x\n",
        "        delta_norms = torch.norm(delta.view(batch_size, -1), p=2, dim=1)\n",
        "        # Scaling factor to ensure ||delta||_2 <= eps\n",
        "        factor = eps / delta_norms\n",
        "        factor = torch.min(factor, torch.ones_like(delta_norms))\n",
        "        delta = delta * factor.view(-1, 1, 1, 1)\n",
        "\n",
        "        # Apply perturbation and clip to valid range [0, 1]\n",
        "        adv_x = torch.clamp(x + delta, 0, 1).detach()\n",
        "    return adv_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Mja_AB4RykO"
      },
      "source": [
        "# Evaluate Single and Multi-Norm Robust Accuracy\n",
        "\n",
        "In this section, we evaluate the model on the Linf and L2 attacks as well as union accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model_on_single_attack(model, attack='pgd_linf', eps=0.1):\n",
        "    model.eval()\n",
        "    tot_test, tot_acc = 0.0, 0.0\n",
        "    ground_acc = 0\n",
        "    for batch_idx, (x_batch, y_batch) in tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Evaluating\"):\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        if attack == 'pgd_linf':\n",
        "            # TODO: get x_adv untargeted pgd linf with eps, and eps_step=eps/4\n",
        "            adv_x = pgd_linf_untargeted(model, x_batch, y_batch, 1, eps, eps/4)\n",
        "            \n",
        "        elif attack == 'pgd_l2':\n",
        "            # TODO: get x_adv untargeted pgd l2 with eps, and eps_step=eps/4\n",
        "            adv_x = pgd_l2_untargeted(model, x_batch, y_batch, 1, eps, eps/4)\n",
        "        else:\n",
        "            pass\n",
        "        \n",
        "        out = model(adv_x)\n",
        "        pred = torch.max(out, dim=1)[1]\n",
        "        # get the testing accuracy and update tot_test and tot_acc\n",
        "        tot_acc += (pred == y_batch).sum().item()\n",
        "        tot_test += y_batch.size(0)\n",
        "\n",
        "        ground_out = model(x_batch)\n",
        "        ground_pred = torch.max(ground_out, dim=1)[1]\n",
        "        ground_acc += (ground_pred == y_batch).sum().item()\n",
        "        \n",
        "            \n",
        "    print('Robust accuracy %.5lf' % (tot_acc/tot_test), f'on {attack} attack with eps = {eps}')\n",
        "    print('Standard accuracy %.5lf' % (ground_acc/tot_test), f'on {attack} attack with eps = {eps}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPMdfEhtR3zm"
      },
      "source": [
        "## Single-Norm Robust Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2823235/3045545859.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('models/pretr_RAMP.pth'))\n",
            "Evaluating: 100%|██████████| 157/157 [00:04<00:00, 37.46it/s]\n",
            "/tmp/ipykernel_2823235/3045545859.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"adversarial_model_eps4.pth\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust accuracy 0.24600 on pgd_linf attack with eps = 0.03137254901960784\n",
            "Standard accuracy 0.37920 on pgd_linf attack with eps = 0.03137254901960784\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [00:04<00:00, 38.33it/s]\n",
            "/tmp/ipykernel_2823235/3045545859.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"adversarial_model_eps8.pth\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust accuracy 0.35800 on pgd_linf attack with eps = 0.03137254901960784\n",
            "Standard accuracy 0.41860 on pgd_linf attack with eps = 0.03137254901960784\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [00:04<00:00, 38.02it/s]\n",
            "/tmp/ipykernel_2823235/3045545859.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"adversarial_model_eps16.pth\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust accuracy 0.31970 on pgd_linf attack with eps = 0.03137254901960784\n",
            "Standard accuracy 0.35790 on pgd_linf attack with eps = 0.03137254901960784\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [00:04<00:00, 37.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust accuracy 0.27080 on pgd_linf attack with eps = 0.03137254901960784\n",
            "Standard accuracy 0.28870 on pgd_linf attack with eps = 0.03137254901960784\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# # Evaluate on Linf attack with different models with eps = 8/255\n",
        "# model.load_state_dict(torch.load('models/pretr_Linf.pth'))\n",
        "# # Evaluate on Linf attack with model 1 with eps = 8/255\n",
        "# test_model_on_single_attack(model, 'pgd_linf', eps=8/255)\n",
        "\n",
        "model.load_state_dict(torch.load('models/pretr_RAMP.pth'))\n",
        "# Evaluate on Linf attack with model 3 with eps = 8/255\n",
        "test_model_on_single_attack(model, 'pgd_linf', eps=8/255)\n",
        "\n",
        "model.load_state_dict(torch.load(\"adversarial_model_eps4.pth\"))\n",
        "test_model_on_single_attack(model, 'pgd_linf', eps=8/255)\n",
        "model.load_state_dict(torch.load(\"adversarial_model_eps8.pth\"))\n",
        "test_model_on_single_attack(model, 'pgd_linf', eps=8/255)\n",
        "model.load_state_dict(torch.load(\"adversarial_model_eps16.pth\"))\n",
        "test_model_on_single_attack(model, 'pgd_linf', eps=8/255)\n",
        "\n",
        "\n",
        "# model.load_state_dict(torch.load('models/pretr_L2.pth'))\n",
        "# # Evaluate on Linf attack with model 2 with eps = 8/255\n",
        "# test_model_on_single_attack(model, 'pgd_linf', eps=8/255)\n",
        "\n",
        "# model.load_state_dict(torch.load('models/pretr_RAMP.pth'))\n",
        "# # Evaluate on Linf attack with model 3 with eps = 8/255\n",
        "# test_model_on_single_attack(model, 'pgd_linf', eps=8/255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2823235/132754123.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('models/pretr_RAMP.pth'))\n",
            "Evaluating: 100%|██████████| 157/157 [00:04<00:00, 36.80it/s]\n",
            "/tmp/ipykernel_2823235/132754123.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"adversarial_model_eps4.pth\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust accuracy 0.31560 on pgd_l2 attack with eps = 0.75\n",
            "Standard accuracy 0.37920 on pgd_l2 attack with eps = 0.75\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [00:04<00:00, 37.73it/s]\n",
            "/tmp/ipykernel_2823235/132754123.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"adversarial_model_eps8.pth\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust accuracy 0.39050 on pgd_l2 attack with eps = 0.75\n",
            "Standard accuracy 0.41860 on pgd_l2 attack with eps = 0.75\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [00:04<00:00, 37.75it/s]\n",
            "/tmp/ipykernel_2823235/132754123.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"adversarial_model_eps16.pth\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust accuracy 0.34190 on pgd_l2 attack with eps = 0.75\n",
            "Standard accuracy 0.35790 on pgd_l2 attack with eps = 0.75\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [00:04<00:00, 37.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust accuracy 0.28110 on pgd_l2 attack with eps = 0.75\n",
            "Standard accuracy 0.28870 on pgd_l2 attack with eps = 0.75\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on L2 attack with different models with eps = 0.75\n",
        "# model.load_state_dict(torch.load('models/pretr_Linf.pth'))\n",
        "# # Evaluate on Linf attack with model 1 with eps = 0.75\n",
        "# test_model_on_single_attack(model, 'pgd_l2', eps=0.75)\n",
        "\n",
        "# model.load_state_dict(torch.load('models/pretr_L2.pth'))\n",
        "# # Evaluate on Linf attack with model 2 with eps = 0.75\n",
        "# test_model_on_single_attack(model, 'pgd_l2', eps=0.75)\n",
        "\n",
        "model.load_state_dict(torch.load('models/pretr_RAMP.pth'))\n",
        "# Evaluate on Linf attack with model 3 with eps = 0.75\n",
        "test_model_on_single_attack(model, 'pgd_l2', eps=0.75)\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load(\"adversarial_model_eps4.pth\"))\n",
        "test_model_on_single_attack(model, 'pgd_l2', eps=0.75)\n",
        "model.load_state_dict(torch.load(\"adversarial_model_eps8.pth\"))\n",
        "test_model_on_single_attack(model, 'pgd_l2', eps=0.75)\n",
        "model.load_state_dict(torch.load(\"adversarial_model_eps16.pth\"))\n",
        "test_model_on_single_attack(model, 'pgd_l2', eps=0.75)\n",
        "\n",
        "# model.load_state_dict(torch.load('models/pretr_RAMP.pth'))\n",
        "# # Evaluate on Linf attack with model 3 with eps = 0.75\n",
        "# test_model_on_single_attack(model, 'pgd_l2', eps=0.75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training with epsilon = 0.01568627450980392 (4.00/255)\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 782/782 [01:43<00:00,  7.53it/s, loss=1.93, acc=28.4]\n",
            "Epoch 2: 100%|██████████| 782/782 [01:42<00:00,  7.63it/s, loss=1.74, acc=34.6]\n",
            "Epoch 3: 100%|██████████| 782/782 [01:42<00:00,  7.64it/s, loss=1.66, acc=38]  \n",
            "Epoch 4: 100%|██████████| 782/782 [01:42<00:00,  7.63it/s, loss=1.6, acc=40.6] \n",
            "Epoch 5: 100%|██████████| 782/782 [01:41<00:00,  7.67it/s, loss=1.55, acc=42.3]\n",
            "Epoch 6: 100%|██████████| 782/782 [01:42<00:00,  7.64it/s, loss=1.51, acc=43.8]\n",
            "Epoch 7: 100%|██████████| 782/782 [01:42<00:00,  7.65it/s, loss=1.48, acc=45.3]\n",
            "Epoch 8: 100%|██████████| 782/782 [01:41<00:00,  7.68it/s, loss=1.45, acc=46.2]\n",
            "Epoch 9: 100%|██████████| 782/782 [01:45<00:00,  7.41it/s, loss=1.43, acc=47.3]\n",
            "Epoch 10: 100%|██████████| 782/782 [01:49<00:00,  7.11it/s, loss=1.4, acc=48.6] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10: Train Loss = 1.3960, Train Acc = 48.64%\n",
            "\n",
            "============================================================\n",
            "Evaluation Results (trained with eps = 0.01568627450980392)\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating clean: 100%|██████████| 157/157 [00:01<00:00, 83.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Standard Accuracy (clean): 41.86%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating pgd_linf: 100%|██████████| 157/157 [00:39<00:00,  3.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust Accuracy (PGD eps=4/255): 30.02%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating pgd_linf: 100%|██████████| 157/157 [00:38<00:00,  4.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust Accuracy (PGD eps=8/255): 19.05%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating pgd_linf: 100%|██████████| 157/157 [00:38<00:00,  4.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust Accuracy (PGD eps=16/255): 3.80%\n",
            "\n",
            "Model saved as adversarial_model_eps4.pth\n",
            "\n",
            "============================================================\n",
            "Training with epsilon = 0.03137254901960784 (8.00/255)\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 782/782 [01:51<00:00,  7.02it/s, loss=2.1, acc=22]   \n",
            "Epoch 2: 100%|██████████| 782/782 [01:44<00:00,  7.46it/s, loss=1.96, acc=26.8]\n",
            "Epoch 3: 100%|██████████| 782/782 [01:35<00:00,  8.16it/s, loss=1.91, acc=28.7]\n",
            "Epoch 4: 100%|██████████| 782/782 [01:35<00:00,  8.17it/s, loss=1.88, acc=30.1]\n",
            "Epoch 5: 100%|██████████| 782/782 [01:35<00:00,  8.19it/s, loss=1.87, acc=30.6]\n",
            "Epoch 6: 100%|██████████| 782/782 [01:35<00:00,  8.20it/s, loss=1.84, acc=31.4]\n",
            "Epoch 7: 100%|██████████| 782/782 [01:35<00:00,  8.21it/s, loss=1.82, acc=32.1]\n",
            "Epoch 8: 100%|██████████| 782/782 [01:41<00:00,  7.71it/s, loss=1.8, acc=33]   \n",
            "Epoch 9: 100%|██████████| 782/782 [01:42<00:00,  7.62it/s, loss=1.78, acc=33.6]\n",
            "Epoch 10: 100%|██████████| 782/782 [01:42<00:00,  7.64it/s, loss=1.77, acc=34.1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10: Train Loss = 1.7728, Train Acc = 34.14%\n",
            "\n",
            "============================================================\n",
            "Evaluation Results (trained with eps = 0.03137254901960784)\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating clean: 100%|██████████| 157/157 [00:01<00:00, 89.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Standard Accuracy (clean): 35.79%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating pgd_linf: 100%|██████████| 157/157 [00:36<00:00,  4.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust Accuracy (PGD eps=4/255): 27.98%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating pgd_linf: 100%|██████████| 157/157 [00:35<00:00,  4.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust Accuracy (PGD eps=8/255): 20.58%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating pgd_linf: 100%|██████████| 157/157 [00:35<00:00,  4.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust Accuracy (PGD eps=16/255): 8.78%\n",
            "\n",
            "Model saved as adversarial_model_eps8.pth\n",
            "\n",
            "============================================================\n",
            "Training with epsilon = 0.06274509803921569 (16.00/255)\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 782/782 [01:42<00:00,  7.66it/s, loss=2.25, acc=16.8]\n",
            "Epoch 2: 100%|██████████| 782/782 [01:42<00:00,  7.65it/s, loss=2.15, acc=20.6]\n",
            "Epoch 3: 100%|██████████| 782/782 [01:42<00:00,  7.64it/s, loss=2.14, acc=20.9]\n",
            "Epoch 4: 100%|██████████| 782/782 [01:42<00:00,  7.64it/s, loss=2.14, acc=21]  \n",
            "Epoch 5: 100%|██████████| 782/782 [01:41<00:00,  7.67it/s, loss=2.14, acc=21.1]\n",
            "Epoch 6: 100%|██████████| 782/782 [01:42<00:00,  7.66it/s, loss=2.13, acc=21.3]\n",
            "Epoch 7: 100%|██████████| 782/782 [01:42<00:00,  7.64it/s, loss=2.13, acc=21.1]\n",
            "Epoch 8: 100%|██████████| 782/782 [01:42<00:00,  7.61it/s, loss=2.13, acc=21.1]\n",
            "Epoch 9: 100%|██████████| 782/782 [01:42<00:00,  7.62it/s, loss=2.13, acc=21.6]\n",
            "Epoch 10: 100%|██████████| 782/782 [01:42<00:00,  7.64it/s, loss=2.13, acc=21.2]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10: Train Loss = 2.1259, Train Acc = 21.23%\n",
            "\n",
            "============================================================\n",
            "Evaluation Results (trained with eps = 0.06274509803921569)\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating clean: 100%|██████████| 157/157 [00:01<00:00, 90.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Standard Accuracy (clean): 28.87%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating pgd_linf: 100%|██████████| 157/157 [00:35<00:00,  4.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust Accuracy (PGD eps=4/255): 25.39%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating pgd_linf: 100%|██████████| 157/157 [00:35<00:00,  4.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust Accuracy (PGD eps=8/255): 22.42%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating pgd_linf: 100%|██████████| 157/157 [00:35<00:00,  4.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust Accuracy (PGD eps=16/255): 16.60%\n",
            "\n",
            "Model saved as adversarial_model_eps16.pth\n",
            "\n",
            "============================================================\n",
            "SUMMARY OF RESULTS\n",
            "============================================================\n",
            "Training eps = 4/255: Standard Accuracy = 41.86%\n",
            "Training eps = 8/255: Standard Accuracy = 35.79%\n",
            "Training eps = 16/255: Standard Accuracy = 28.87%\n",
            "\n",
            "Observations:\n",
            "1. Larger epsilon values during training typically lead to:\n",
            "   - Lower standard accuracy on clean examples\n",
            "   - Higher robust accuracy against adversarial attacks\n",
            "2. There's a tradeoff between robustness and accuracy\n",
            "3. The model should be most robust at the epsilon it was trained on\n"
          ]
        }
      ],
      "source": [
        "# Adversarial training function\n",
        "def adversarial_train(model, train_loader, optimizer, epoch, eps, eps_step, k=10):\n",
        "    \"\"\"\n",
        "    Perform one epoch of adversarial training\n",
        "    \n",
        "    Args:\n",
        "        model: neural network model\n",
        "        train_loader: training data loader\n",
        "        optimizer: optimizer\n",
        "        epoch: current epoch number\n",
        "        eps: perturbation budget for PGD\n",
        "        eps_step: step size for PGD\n",
        "        k: number of PGD steps\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    ce_loss = torch.nn.CrossEntropyLoss()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # Generate adversarial examples\n",
        "        adv_data = pgd_linf_untargeted(model, data, target, k, eps, eps_step)\n",
        "        \n",
        "        # Train on adversarial examples\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(adv_data)\n",
        "        loss = ce_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Track statistics\n",
        "        train_loss += loss.item()\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "        total += target.size(0)\n",
        "        \n",
        "        pbar.set_postfix({\n",
        "            'loss': train_loss / (batch_idx + 1),\n",
        "            'acc': 100. * correct / total\n",
        "        })\n",
        "    \n",
        "    return train_loss / len(train_loader), 100. * correct / total\n",
        "\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, test_loader, eps=0.0, attack_type='clean', k=10):\n",
        "    \"\"\"\n",
        "    Evaluate model on clean or adversarial examples\n",
        "    \n",
        "    Args:\n",
        "        model: neural network model\n",
        "        test_loader: test data loader\n",
        "        eps: perturbation budget (0 for clean evaluation)\n",
        "        attack_type: 'clean' or 'pgd_linf'\n",
        "        k: number of PGD steps for attack\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad() if attack_type == 'clean' else torch.enable_grad():\n",
        "        for data, target in tqdm(test_loader, desc=f'Evaluating {attack_type}'):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            if attack_type == 'pgd_linf' and eps > 0:\n",
        "                data = pgd_linf_untargeted(model, data, target, k, eps, eps/4)\n",
        "            \n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "    \n",
        "    accuracy = 100. * correct / total\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Main training and evaluation\n",
        "def train_and_evaluate(eps_train, num_epochs=50):\n",
        "    \"\"\"\n",
        "    Train model with adversarial training and evaluate\n",
        "    \n",
        "    Args:\n",
        "        eps_train: epsilon value for adversarial training\n",
        "        num_epochs: number of training epochs\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training with epsilon = {eps_train} ({eps_train*255:.2f}/255)\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    # Initialize model\n",
        "    model = PreActResNet18(10, cuda=True, activation='softplus1', normal='cifar10').to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[40, 45], gamma=0.1)\n",
        "    \n",
        "    # Training\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_loss, train_acc = adversarial_train(\n",
        "            model, train_loader, optimizer, epoch, \n",
        "            eps=eps_train, eps_step=eps_train/4, k=10\n",
        "        )\n",
        "        scheduler.step()\n",
        "        \n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"\\nEpoch {epoch}: Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.2f}%\")\n",
        "    \n",
        "    # Evaluation\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Evaluation Results (trained with eps = {eps_train})\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Standard accuracy (clean examples)\n",
        "    clean_acc = evaluate(model, test_loader, eps=0.0, attack_type='clean')\n",
        "    print(f\"Standard Accuracy (clean): {clean_acc:.2f}%\")\n",
        "    \n",
        "    # Robust accuracy at different epsilon values\n",
        "    test_epsilons = [4/255, 8/255, 16/255]\n",
        "    for eps_test in test_epsilons:\n",
        "        robust_acc = evaluate(model, test_loader, eps=eps_test, attack_type='pgd_linf', k=20)\n",
        "        print(f\"Robust Accuracy (PGD eps={eps_test*255:.0f}/255): {robust_acc:.2f}%\")\n",
        "    \n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), f'adversarial_model_eps{int(eps_train*255)}.pth')\n",
        "    print(f\"\\nModel saved as adversarial_model_eps{int(eps_train*255)}.pth\")\n",
        "    \n",
        "    return model, clean_acc\n",
        "\n",
        "\n",
        "# Run experiments with different epsilon values\n",
        "\n",
        "# Training epsilon values to test\n",
        "epsilon_values = [4/255, 8/255, 16/255]\n",
        "\n",
        "results = {}\n",
        "for eps in epsilon_values:\n",
        "    model, clean_acc = train_and_evaluate(eps, num_epochs=10)\n",
        "    results[eps] = clean_acc\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"SUMMARY OF RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "for eps, acc in results.items():\n",
        "    print(f\"Training eps = {eps*255:.0f}/255: Standard Accuracy = {acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-Norm Robust Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model_on_multi_attacks(model, eps_linf=8./255., eps_l2=0.75):\n",
        "    model.eval()\n",
        "    tot_test, tot_acc = 0.0, 0.0\n",
        "    ground_acc = 0.0\n",
        "    for batch_idx, (x_batch, y_batch) in tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Evaluating\"):\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        # TODO: get x_adv_linf and x_adv_l2 untargeted pgd linf and l2 with eps, and eps_step=eps/4\n",
        "        x_adv_linf = pgd_linf_untargeted(model, x_batch, y_batch, 10, eps_linf, eps_linf/4)\n",
        "        x_adv_l2 = pgd_l2_untargeted(model, x_batch, y_batch, 10, eps_l2, eps_l2/4)\n",
        "        \n",
        "        ## calculate union accuracy: correct only if both attacks are correct\n",
        "        \n",
        "        out = model(x_adv_linf)\n",
        "        pred_linf = torch.max(out, dim=1)[1]\n",
        "        out = model(x_adv_l2)\n",
        "        pred_l2 = torch.max(out, dim=1)[1]\n",
        "\n",
        "        ground_out = model(x_batch)\n",
        "        ground_pred = torch.max(ground_out, dim=1)[1]\n",
        "        ground_acc += (ground_pred == y_batch).sum().item()\n",
        "        \n",
        "        # TODO: get the testing accuracy with multi-norm robustness and update tot_test and tot_acc\n",
        "        tot_acc += ((pred_linf == y_batch) & (pred_l2 == y_batch)).sum().item()\n",
        "        tot_test += y_batch.size(0)\n",
        "            \n",
        "    print('Robust accuracy %.5lf' % (tot_acc/tot_test), f'on multi attacks')\n",
        "    print('Standard accuracy %.5lf' % (ground_acc/tot_test), f'on multi attacks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2608180/2844882111.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('models/pretr_Linf.pth'))\n",
            "Evaluating: 100%|██████████| 157/157 [00:28<00:00,  5.43it/s]\n",
            "/tmp/ipykernel_2608180/2844882111.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('models/pretr_L2.pth'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust accuracy 0.51200 on multi attacks\n",
            "Standard accuracy 0.82800 on multi attacks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [00:29<00:00,  5.41it/s]\n",
            "/tmp/ipykernel_2608180/2844882111.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('models/pretr_RAMP.pth'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust accuracy 0.30860 on multi attacks\n",
            "Standard accuracy 0.88760 on multi attacks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [00:29<00:00,  5.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robust accuracy 0.49740 on multi attacks\n",
            "Standard accuracy 0.81190 on multi attacks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on multi-norm attacks with different models with eps_linf = 8./255, eps_l2 = 0.75\n",
        "model.load_state_dict(torch.load('models/pretr_Linf.pth'))\n",
        "# Evaluate on multi attacks with model 1\n",
        "test_model_on_multi_attacks(model, eps_linf=8./255., eps_l2=0.75)\n",
        "\n",
        "model.load_state_dict(torch.load('models/pretr_L2.pth'))\n",
        "# Evaluate on multi attacks with model 2\n",
        "test_model_on_multi_attacks(model, eps_linf=8./255., eps_l2=0.75)\n",
        "\n",
        "model.load_state_dict(torch.load('models/pretr_RAMP.pth'))\n",
        "# Evaluate on multi attacks with model 3\n",
        "test_model_on_multi_attacks(model, eps_linf=8./255., eps_l2=0.75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pensieve",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
